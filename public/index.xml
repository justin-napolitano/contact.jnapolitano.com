<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Justin Napolitano</title>
    <link>jnapolitano.com/</link>
    <description>Recent content on Justin Napolitano</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>jayburdindustries</copyright>
    <lastBuildDate>Sun, 16 Jun 2024 21:35:34 -0500</lastBuildDate><atom:link href="jnapolitano.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prepare Chicken Salad</title>
      <link>jnapolitano.com/posts/prepare-chicken-salad/</link>
      <pubDate>Sun, 16 Jun 2024 21:35:34 -0500</pubDate>
      
      <guid>jnapolitano.com/posts/prepare-chicken-salad/</guid>
      <description>Why I am hungry. I have a bunch of ingredients. I have a left over rotisserie chicken. I do not want to waste.
Audit your supplies In my case I have at my disposal
 About half a rotisserie chicken. Greek yogurt. Coriander. Grape tomatotes. Honey. Dates.  Prepare your food Cut up the chicken breast Just remove chicken breast. Then cut it up into small chunks.
Cut up the herbs I am using left-over coriander leaves to add some vitamins and flavor to my dinner.</description>
    </item>
    
    <item>
      <title>Automate Posting Hugo Blog to Social Sites...Second Attempt</title>
      <link>jnapolitano.com/posts/python-rss-reader/</link>
      <pubDate>Sat, 15 Jun 2024 22:36:34 -0500</pubDate>
      
      <guid>jnapolitano.com/posts/python-rss-reader/</guid>
      <description>Thoughts on This Second Pass  I will create a script that parses the sites rss feed&amp;hellip; it will then traverse the xml tree entries&amp;hellip; if a date is newer than the last publish date&amp;hellip; publish that post&amp;hellip;  I am still thinking through how to publish. I will likely write a monolithic script here, but ideally I would write an api or a batch processor to handle this in some way.</description>
    </item>
    
    <item>
      <title>Automate Posting Hugo Blog to Social Sites... Failure</title>
      <link>jnapolitano.com/posts/hugo-social-publisher/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/hugo-social-publisher/</guid>
      <description>Why I have a hugo blog that is a pian to share across my social feeds. I want to automate it.
Create a mockup For this I quickly sketched out my thoughts onto a writing pad. My thinking is that I will drop a yaml file into each post directory to be read by a a python application calling social apis.
Mockup As you can see this is very rudimentary. I will traverse the posts directories looking for a publish.</description>
    </item>
    
    <item>
      <title>JB &#39;Welding&#39; My Bumper</title>
      <link>jnapolitano.com/posts/accord-bumper-weld/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/accord-bumper-weld/</guid>
      <description>So, I snagged my already hanging on by a thread bumper on a piece of rebar. I reversed thinking it was a small bump in the parking lot&amp;hellip; I was wrong.
Why Bumpers are expensive. I am looking at around $400.00 of bumper + paint if I try to do this on my own. Taking it to the shop&amp;hellip; yeah I won&amp;rsquo;t do that. So, rather than trying to make the prettiest beater in the car lot, I am going to make it at least a functional beater.</description>
    </item>
    
    <item>
      <title>Justin-ify a Car Headliner</title>
      <link>jnapolitano.com/posts/accord-headliner/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/accord-headliner/</guid>
      <description>My beautiful 12 year old Honda has a bad headliner. It was gross. I did not like it.
Why The foam holding it in place to the headliner board deteriorated. The car is too old to make it perfect so any headliner fabric will work.
Find a suitable headliner replacement fabric I live in Texas and its hot. Really hot. Without some insulation the heat from the sun would bake me inside my car.</description>
    </item>
    
    <item>
      <title>Hugo Build and Deploy GH Workflow</title>
      <link>jnapolitano.com/posts/gh-pages-workflow/</link>
      <pubDate>Tue, 11 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/gh-pages-workflow/</guid>
      <description>Creating a GH Workflow to Build and Deploy a hugo site to gh-pages Why To simplify the build process.
Creating the Workflow create your yaml config file touch hugo.yaml Set the trigger and the environment defaults The code below creates a trigger on push from the main and the gh-pages branches. It also sets read and write permissions to permit executing code and building hugo.
on:# Runs on pushes targeting the default branchpush:branches:- main# - pit# - ghpages- gh-pages# Allows you to run this workflow manually from the Actions tabworkflow_dispatch:# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pagespermissions:contents:readpages:writeid-token:write# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.</description>
    </item>
    
    <item>
      <title>GCP Cloud Run Job Scraper</title>
      <link>jnapolitano.com/posts/loc_scraper/</link>
      <pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/loc_scraper/</guid>
      <description>Library of Congress Scraper Job This repo scrapes the library of congress for all of the US Supreme Court Cases available on their platform. I intent to use this data to create a research tool to better understand the corpus of text.
Quick History of this project I had started work on this as an undergraduate at university, but the chatbot apis were not yet available.. and training modesl were far too expensive.</description>
    </item>
    
    <item>
      <title>Creating a GCP Client Tool</title>
      <link>jnapolitano.com/posts/gcputils/</link>
      <pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/gcputils/</guid>
      <description>Creating a GCP Client Tool in Python Why I often find myself reuuing the same bits of code when working with GCP. It is very important to avoid creating multiple development trees of the same classes. I have done this before for large projects. It will lead to a very difficult to maintain stack of tools that will spaghettify over time.
Creating the submodule Create an empty directory mkdir gcpuptils Add the following code to a gcpclient.</description>
    </item>
    
    <item>
      <title>Model Design and Logistic Regression in Python</title>
      <link>jnapolitano.com/posts/logistic_regression_mockup/</link>
      <pubDate>Fri, 17 Jun 2022 13:20:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/logistic_regression_mockup/</guid>
      <description>Model Design and Logistic Regression in Python I recently modeled customer churn in Julia with logistic regression model. It was interesting to be sure, but I want to extend my analysis skillset by modeling biostatistics data. In this post, I design a logistic regression model of health predictors.
Imports # load some default Python modules import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns plt.</description>
    </item>
    
    <item>
      <title>Annual Cost of Living Monte Carlo Models</title>
      <link>jnapolitano.com/posts/cost-of-living-projections/</link>
      <pubDate>Wed, 01 Jun 2022 15:24:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/cost-of-living-projections/</guid>
      <description>Cost of Living Projections Introduction I do not like negotiating for salary. Especially, without valid projections to determine a range.
I prepared this report to estimate a salary expectation that will maintain my current standard of living.
I present two Monte Carlo models of Houston and NYC annual living costs. The data is somewhat dated and &amp;ndash;particularly in the case of houston&amp;ndash; are high level estimates.
In order to produce a better report, I am currently scraping data from the internet for more accurate sample distributions.</description>
    </item>
    
  </channel>
</rss>
